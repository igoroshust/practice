"""Парсер, подтягивающий вопросы из stackoverflow (bs4)"""
# from bs4 import BeautifulSoup
# import requests
#
# base = 'https://ru.stackoverflow.com'
#
# html = requests.get(base).content # получаем html-код страницы целиком
#
# soup = BeautifulSoup(html, 'lxml') # создаём объект супа. Первый аргумент в конструкторе - это весь html код. Второй аргумент - сама библиотека для парсинга (в нашем случае lxml).
#
# div = soup.find('div', id='question-mini-list') # находим с помощью метода find() нужный див, уточняя ID
#
# a = div.find_all('a', class_='s-link') # ищем только нужные нам объекты тегов <a>. Метод find_all() возвращает список объектов bs, все из которых удовлетворяют нашим критериям.
#
#
# for _ in a:
#     print(_.getText(), base + _.get('href')) # печатаем название вопроса и ссылка на него в консоль
#     # метод getText() возвращает текст внутри тега (это и будет названием вопроса)
#     # чтобы получить атрибут тега (ссылка лежит в атрибуте href) можно обратиться к неу как к словарю через [] или же использовать метод .get()
#
# # a = div.find('a', class_='s-link') # ищем только нужные нам объекты тегов <a>. Метод find_all() возвращает список объектов bs, все из которых удовлетворяют нашим критериям.
# #
# # parent = a.find_parent() # поиск родительского элемента
# # print(parent)


"""Парсер (подтягиваем данные из сохранённого документа)"""
# import requests # отправление запросов на сервер и получение ответа
# import lxml.html # обработка данных HTML и XML
# from lxml import etree # модуль etree позволяет создавать элементы XML/HTML и их подэлементы (полезно, если пытаемся написать XML/HTML-файлы или манипулировать ими)
#
# # создадим объект ElementTree. Он возвращается функцией parse()
# tree = etree.parse('Welcome to Python.org.html', lxml.html.HTMLParser()) # попытаемся спарсить файл с помощью html-парсера.
#
# ul = tree.findall('//*[@id="content"]/div/section/div[2]/div[1]/div/ul/li') # помещаем в аргумент метода findall скопированный xpath. Здесь мы получим все элементы списка новостей (все заголовки и их даты). Findall возвращает список многих подходящих элементов
#
# for li in ul:
#     """Создаём цикл, в котором будем выводить название каждого элемента из списка"""
#     a = li.find('a') # в каждом элементе находим расположение заголовка новости, у нас это тег <a> (т.е. гиперссылка). Метод find возвращает первый подходящий элемент
#     time = li.find('time')
#     print(f"""{time.get('datetime')} --- {a.text}""")


"""Парсер"""
# import requests # отправление запросов на сервер и получение ответа
# import lxml.html # обработка данных HTML и XML
# from lxml import etree # модуль etree позволяет создавать элементы XML/HTML и их подэлементы (полезно, если пытаемся написать XML/HTML-файлы или манипулировать ими)
#
# html = requests.get('https://www.python.org/').content # получаем html главной страницы официального сайта python, content используется для получения содержимого страницы (он позволяет получить информацию в виде байтов, то есть в итоге у нас будет вся информация, не только строковая).
#
# tree = lxml.html.document_fromstring(html) # анализирует документ по заданной строке. При этом всегда создаётся правильный html-документ, что означает, что родительским узлом ялвяется <html>, а также есть тело и, возможно, заголовок.
# title = tree.xpath('/html/body/div/div[3]/div/section/div[2]/div[1]/div/ul/li[1]/a/text()')
#
# # title = tree.xpath('/html/head/title/text()') # забираем текст тега <title> из тега <head>, который лежит в свою очередь внутри тега <html>
#
# print(title)
#### Введение в алгоритмы
Алгоритм - последовательность шагов (инструкций), которая решает конкретную задачу. Это фундаментальная часть программирования, 
благодаря которой можно найти способы эффективного решения задач. Аналогично рецепту приготовления еды: ингредиенты + шаги = результат.
Алгоритмы используются везде: от поиска в Google до игр и ИИ.

**Ключевые свойства алгоритма (ВЭКО):**
- Определённость. Каждый шаг чёткий, без неоднозначностей;
- Конечность. Алгоритм всегда завершается за конечное число шагов;
- Эффективность. Необходимо стремиться к минимальному использованию ресурсов (времени и памяти);
- Вход и выход: алгоритм принимает данные (вход) и выдаёт результат (выход).

**Базовые понятия: сложность алгоритма**
Для измерения эффективности используется Big O notation (О-нотация). Она описывает, как растёт время выполнения или память
в зависимости от размера входных данных.

- O(1): Постоянное время (не зависит от n). Например, доступ к элементу списка по индексу;
- O(n): Линейнеое время (проходим весь список один раз);
- O(n^2): Квадратичное время (вложенные циклы, например, для N элементов проверяем каждый с каждым)
- O(log n): Логарифмическое (быстрое, как бинарный поиск)

#### Пример на Python: Простая функция (O(1))
Функция возврата первого элемента списка (постоянное время).
```python
def get_first_element(lst):
    """Получение первого элемента списка"""
    if lst:  # Проверяем, не пустой ли список
        return lst[0]  # Доступ по индексу - O(1)
    return None


# Тестируем
my_list = [1, 2, 3, 4, 5]
print(get_first_element(my_list))
```
Независимо от размера списка, мы сразу берём элемент с индексом 0. Это быстро.

#### Пример для O(n): линейное время (проходим весь список один раз)
Линейная сложность возникает, когда алгоритм обрабатывает каждый элемент ровно один раз, без вложенных циклов.
Это эффективно для больших данных, но время растёт пропорционально n.
Пример: подсчёт сумму элементов в списке. Мы проходим список один раз, добавляя значения.

Сложность (O(n)) - для n элементов выполняем n операций сложения.

Функция подсчёта суммы элементов
```python
def sum_list(arr):
    """Подсчёт суммы"""
    total = 0  # Инициализация суммы
    for num in arr:  # Цикл проходит каждый элемент один раз
        total += num  # Добавляем к сумме
    return total


# Тестирование
numbers = [1, 2, 3, 4, 5]
result = sum_list(numbers)
print(f'Сумма элементов: {result}')  # Вывод: 15


# Для большого списка (n=1000)
large_list = list(range(1, 1001))  # [1, 2, ..., 1000]
large_sum = sum_list(large_list)
print(f'Сумма большого списка (n=1000): {large_sum}')  # Вывод: 500500
```

Почему этот пример O(n)? Время выполнения напрямую зависит от n: для n=10 - 10 операций, для n=1000000 - млн. 
При этом нет вложенности, так что не растёт быстрее. 

Дополнительно:
Функция sum(arr) тоже O(n). Это базовый пример для задач вроде "подсчёта среднего значения" или "проверки на наличие элемента"
(как в линейном поиске).

O(n) используется для задач, где нужно обработать все данные, но без сравнений между элементами.


#### Пример для O(n^2): квадратичное время (вложенные циклы, проверяем каждый с каждым)
Квадратичный алгоритм - время выполнения алгоритма растёт пропорционально квадрату размера входных данных.
Такие алгоритмы просты в реализации, но их применяют только для небольших данных или как часть более сложных структур.
Пример: простые сортировки (пузырьковая, вставками), поиск дубликатов в списке без оптимизаций.

Квадратичная сложность появляется при вложенных циклах: внешний цикл n раз, внутренний - тоже ~n раз, итого получается:
n x n = n^2 операций. Это подходит для маленьких n (до 1000), но для больших (миллионы) становится слишком медленно.
Пример: проверка на дубликаты в списке простым способом - сравниваем каждый элемент со всеми остальными.

Сложность: (O(n^2)) - в худшем случае (все уникальные) проверяем все пары.

Пример поиска дубликатов
```python
def find_duplicates(arr):
    """Поиск дубликатов"""
    duplicates = []  # Список для хранения дубликатов
    for i in range(len(arr)):  # Внешний цикл: n итераций
        for j in range(i + 1, len(arr)):  # Внутренний: ~n/2 в среднем, но O(n)
            if arr[i] == arr[j] and arr[i] not in duplicates:
                duplicates.append(arr[i])  # Добавляем, если нашли пару
    return duplicates


# Тестирование
numbers = [1, 2, 3, 2, 4, 5, 3, 6]
result = find_duplicates(numbers)
print(f'Дубликаты: {result}')  # Вывод: [2, 3]

# Для списка без дубликатов (худший случай O(n^2))
unique_numbers = [1, 2, 3, 4, 5, 6, 7, 8]
result_unique = find_duplicates(unique_numbers)
print(f'Дубликаты в уникальном списке: {result_unique}')  # Вывод: []
```

Почему это O(n^2)? Количество сравнений: для n=5 - 10 пар (1-2, 1-3,..., 4-5). В общем: n(n-1)/2 ~ O(n^2). Для n = 1000 - 
~ 500 000 операций (ещё приемлемо), но для n = 10 000 - 50 млн (замедлит)

Практика: Это как "простая проверка на уникальность". В реальности нужно использовать set для O(n), а в примере выше показано,
почему вложенные циклы дороги. Другие примеры: умножение матриц n*n или "нахождение всех пар суммой К"

Когда использовать: только для обучения или очень малых данных. Нужно оптимизировать: использовать хэш-таблицы (O(n))

#### Пример для O(log n): логарифмическое время (быстрое, как бинарный поиск)
Логарифмическая сложность - когда на каждом шаге размер проблемы уменьшается вдвое (или в k раз). Это очень эффективно:
для n = 1000000 шагов всего ~20 (поскольку 2^20 = 1000000).

Пример: нахождение наибольшей степени двойки, меньшей или равной n (используя бинарное представление или цикл деления).
Это похоже на бинарный поиск, но для числа. Сложность: O(log n) - количество итераций равно log2n

```python
def largest_power_of_two(n):
    """Наибольшее число со степенью 2 меньше n"""
    if n < 1:
        return 0
    power = 1  # Начинаем с 2^0 = 1
    while power * 2 <= n:  # Пока удвоение не превысит n
        power *= 2  # Удваиваем (делим пространство поиска пополам)
    return power


# Тестирование
print(
    largest_power_of_two(10),  # Вывод: 8 (2^3 = 8 <= 10)
    largest_power_of_two(1),  # Вывод: 1 (2^0 = 1)
    largest_power_of_two(100),  # Вывод: 64 (2^6 = 64 <= 100)
)

# Для большого n (n=1_000_000)
large_n = 1_000_000
result = largest_power_of_two(large_n)
print(f'Для n = {large_n}: {result}')  # Вывод: 524288 (2^19)

```
Шаги: начинаем с power = 1. В цикле while проверяем, можно ли удвоить (т.е. 2 * power <= n). Если да - удваиваем. 
Цикл останавливается, когда удвоение превысит n.

Почему O(log n)? Каждый шаг удваивает power, что эквивалентно делению n пополам (как в бинарном поиске). 
Количество итераций: log2n. Для n = 1 000 000: 19-20 шагов - быстро.

Практика: это упрощённый "бинарный поиск по экспоненте". В бинарном поиске мы делим массив пополам; здесь - число.

Когда использовать: для отсортированных данных или задач с экспоненциальным ростом (например, в алгоритмах сжатия или графах)

+ Можно измерить время с помощью timeit